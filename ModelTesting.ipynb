{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "fPe4gPnL9YJk"
      ],
      "authorship_tag": "ABX9TyOhbWeUFEO2H2Bhj5+tWPqz"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# AlexNet"
      ],
      "metadata": {
        "id": "fPe4gPnL9YJk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Mounting google drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive/')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C2B2vgQslJgk",
        "outputId": "d095c313-9a72-4f29-d6cc-e5f35617a7bf"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive/\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Extracting the data\n",
        "!unzip \"/content/drive/MyDrive/Major/RanjanaDataset.zip\" -d RanjanaDataset"
      ],
      "metadata": {
        "id": "ScCRwbR1HEzO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow import keras"
      ],
      "metadata": {
        "id": "V8pCwDe_nH4v"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "sUJBJGZ9z17n"
      },
      "outputs": [],
      "source": [
        "from keras.models import Sequential, Model\n",
        "from keras.layers import BatchNormalization\n",
        "from keras.layers import Activation\n",
        "from keras.layers import Flatten\n",
        "from keras.layers import Dropout\n",
        "from keras.layers import Dense\n",
        "from keras.layers import Conv2D, MaxPooling2D\n",
        "from keras import layers\n",
        "\n",
        "\n",
        "# Define the model using AlexNet architecture\n",
        "def model(num_classes):\n",
        "    model = Sequential()\n",
        "\n",
        "     # 1st Convolutional Layer\n",
        "    model.add(Conv2D(filters=32, input_shape=(64,64,1), kernel_size=(3, 3), strides=(1, 1), padding='same'))\n",
        "    model.add(Activation('relu'))\n",
        "    model.add(MaxPooling2D(pool_size=(2, 2), strides=(2, 2), padding='valid'))\n",
        "\n",
        "    # 2nd Convolutional Layer\n",
        "    model.add(Conv2D(filters=64, kernel_size=(3, 3), strides=(1, 1), padding='same'))\n",
        "    model.add(Activation('relu'))\n",
        "    model.add(MaxPooling2D(pool_size=(2, 2), strides=(2, 2), padding='valid'))\n",
        "\n",
        "    # 3rd Convolutional Layer\n",
        "    model.add(Conv2D(filters=128, kernel_size=(3, 3), strides=(1, 1), padding='same'))\n",
        "    model.add(Activation('relu'))\n",
        "\n",
        "    # 4th Convolutional Layer\n",
        "    model.add(Conv2D(filters=128, kernel_size=(3, 3), strides=(1, 1), padding='same'))\n",
        "    model.add(Activation('relu'))\n",
        "\n",
        "    # 5th Convolutional Layer\n",
        "    model.add(Conv2D(filters=64, kernel_size=(3, 3), strides=(1, 1), padding='same'))\n",
        "    model.add(Activation('relu'))\n",
        "    model.add(MaxPooling2D(pool_size=(2, 2), strides=(2, 2), padding='valid'))\n",
        "\n",
        "    # Passing it to a Fully Connected layer\n",
        "    model.add(Flatten())\n",
        "    # 1st Fully Connected Layer\n",
        "    model.add(Dense(256))\n",
        "    model.add(Activation('relu'))\n",
        "    model.add(Dropout(0.5))\n",
        "\n",
        "    # 2nd Fully Connected Layer\n",
        "    model.add(Dense(256))\n",
        "    model.add(Activation('relu'))\n",
        "    model.add(Dropout(0.5))\n",
        "\n",
        "    # Output Layer\n",
        "    model.add(Dense(num_classes))\n",
        "    model.add(Activation('softmax'))\n",
        "\n",
        "    return model"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from keras.optimizers import Adam\n",
        "from keras.callbacks import ReduceLROnPlateau\n",
        "import os\n",
        "\n",
        "# Define constants\n",
        "DATASET_PATH = '/content/RanjanaDataset/Ranjana/Training'\n",
        "MODEL_PATH = '.'\n",
        "BATCH_SIZE = 128\n",
        "EPOCHS = 20\n",
        "TARGET_WIDTH = 64\n",
        "TARGET_HEIGHT = 64\n",
        "TARGET_DEPTH = 1"
      ],
      "metadata": {
        "id": "7hEgeFFv9hgH"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Set up the data generator to flow data from disk\n",
        "print(\"[INFO] Setting up Data Generator...\")\n",
        "data_gen = ImageDataGenerator(validation_split=0.2, rescale=1./255)\n",
        "\n",
        "train_generator = data_gen.flow_from_directory(\n",
        "    DATASET_PATH,\n",
        "    subset='training',\n",
        "    target_size = (TARGET_WIDTH, TARGET_HEIGHT),\n",
        "    batch_size = BATCH_SIZE,\n",
        "    color_mode='grayscale'  # Add this line to convert images to grayscale\n",
        ")\n",
        "\n",
        "val_generator = data_gen.flow_from_directory(\n",
        "    DATASET_PATH,\n",
        "    subset='validation',\n",
        "    target_size = (TARGET_WIDTH, TARGET_HEIGHT),\n",
        "    batch_size = BATCH_SIZE,\n",
        "    color_mode='grayscale'  # Add this line to convert images to grayscale\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MMjcooiO9kdZ",
        "outputId": "320a2ffd-0490-42cc-8bde-ae7131160c9f"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[INFO] Setting up Data Generator...\n",
            "Found 83328 images belonging to 62 classes.\n",
            "Found 20832 images belonging to 62 classes.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Build model\n",
        "print(\"[INFO] Compiling model...\")\n",
        "alexnet = model(train_generator.num_classes)\n",
        "\n",
        "# Compile the model\n",
        "alexnet.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DvLFKopQ9zBb",
        "outputId": "64cca8ff-294e-406b-93e1-5514b19ca12d"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[INFO] Compiling model...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/layers/convolutional/base_conv.py:107: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "alexnet.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "2mfQuqrLssBM",
        "outputId": "f57c01c4-7628-47cf-e18f-c2b39dd680ec"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1mModel: \"sequential\"\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential\"</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n",
              "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                        \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape               \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m        Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n",
              "│ conv2d (\u001b[38;5;33mConv2D\u001b[0m)                      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m, \u001b[38;5;34m64\u001b[0m, \u001b[38;5;34m32\u001b[0m)          │             \u001b[38;5;34m320\u001b[0m │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ activation (\u001b[38;5;33mActivation\u001b[0m)              │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m, \u001b[38;5;34m64\u001b[0m, \u001b[38;5;34m32\u001b[0m)          │               \u001b[38;5;34m0\u001b[0m │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ max_pooling2d (\u001b[38;5;33mMaxPooling2D\u001b[0m)         │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m32\u001b[0m)          │               \u001b[38;5;34m0\u001b[0m │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ conv2d_1 (\u001b[38;5;33mConv2D\u001b[0m)                    │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m64\u001b[0m)          │          \u001b[38;5;34m18,496\u001b[0m │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ activation_1 (\u001b[38;5;33mActivation\u001b[0m)            │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m64\u001b[0m)          │               \u001b[38;5;34m0\u001b[0m │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ max_pooling2d_1 (\u001b[38;5;33mMaxPooling2D\u001b[0m)       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m64\u001b[0m)          │               \u001b[38;5;34m0\u001b[0m │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ conv2d_2 (\u001b[38;5;33mConv2D\u001b[0m)                    │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m128\u001b[0m)         │          \u001b[38;5;34m73,856\u001b[0m │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ activation_2 (\u001b[38;5;33mActivation\u001b[0m)            │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m128\u001b[0m)         │               \u001b[38;5;34m0\u001b[0m │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ conv2d_3 (\u001b[38;5;33mConv2D\u001b[0m)                    │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m128\u001b[0m)         │         \u001b[38;5;34m147,584\u001b[0m │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ activation_3 (\u001b[38;5;33mActivation\u001b[0m)            │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m128\u001b[0m)         │               \u001b[38;5;34m0\u001b[0m │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ conv2d_4 (\u001b[38;5;33mConv2D\u001b[0m)                    │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m64\u001b[0m)          │          \u001b[38;5;34m73,792\u001b[0m │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ activation_4 (\u001b[38;5;33mActivation\u001b[0m)            │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m64\u001b[0m)          │               \u001b[38;5;34m0\u001b[0m │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ max_pooling2d_2 (\u001b[38;5;33mMaxPooling2D\u001b[0m)       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m64\u001b[0m)            │               \u001b[38;5;34m0\u001b[0m │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ flatten (\u001b[38;5;33mFlatten\u001b[0m)                    │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m4096\u001b[0m)                │               \u001b[38;5;34m0\u001b[0m │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dense (\u001b[38;5;33mDense\u001b[0m)                        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)                 │       \u001b[38;5;34m1,048,832\u001b[0m │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ activation_5 (\u001b[38;5;33mActivation\u001b[0m)            │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)                 │               \u001b[38;5;34m0\u001b[0m │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dropout (\u001b[38;5;33mDropout\u001b[0m)                    │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)                 │               \u001b[38;5;34m0\u001b[0m │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dense_1 (\u001b[38;5;33mDense\u001b[0m)                      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)                 │          \u001b[38;5;34m65,792\u001b[0m │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ activation_6 (\u001b[38;5;33mActivation\u001b[0m)            │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)                 │               \u001b[38;5;34m0\u001b[0m │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dropout_1 (\u001b[38;5;33mDropout\u001b[0m)                  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)                 │               \u001b[38;5;34m0\u001b[0m │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dense_2 (\u001b[38;5;33mDense\u001b[0m)                      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m62\u001b[0m)                  │          \u001b[38;5;34m15,934\u001b[0m │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ activation_7 (\u001b[38;5;33mActivation\u001b[0m)            │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m62\u001b[0m)                  │               \u001b[38;5;34m0\u001b[0m │\n",
              "└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n",
              "┃<span style=\"font-weight: bold\"> Layer (type)                         </span>┃<span style=\"font-weight: bold\"> Output Shape                </span>┃<span style=\"font-weight: bold\">         Param # </span>┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n",
              "│ conv2d (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)                      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)          │             <span style=\"color: #00af00; text-decoration-color: #00af00\">320</span> │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ activation (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Activation</span>)              │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)          │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ max_pooling2d (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling2D</span>)         │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)          │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ conv2d_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)                    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)          │          <span style=\"color: #00af00; text-decoration-color: #00af00\">18,496</span> │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ activation_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Activation</span>)            │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)          │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ max_pooling2d_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling2D</span>)       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)          │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ conv2d_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)                    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)         │          <span style=\"color: #00af00; text-decoration-color: #00af00\">73,856</span> │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ activation_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Activation</span>)            │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)         │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ conv2d_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)                    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)         │         <span style=\"color: #00af00; text-decoration-color: #00af00\">147,584</span> │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ activation_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Activation</span>)            │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)         │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ conv2d_4 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)                    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)          │          <span style=\"color: #00af00; text-decoration-color: #00af00\">73,792</span> │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ activation_4 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Activation</span>)            │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)          │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ max_pooling2d_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling2D</span>)       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)            │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ flatten (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Flatten</span>)                    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4096</span>)                │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)                 │       <span style=\"color: #00af00; text-decoration-color: #00af00\">1,048,832</span> │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ activation_5 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Activation</span>)            │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)                 │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dropout (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)                    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)                 │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dense_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)                 │          <span style=\"color: #00af00; text-decoration-color: #00af00\">65,792</span> │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ activation_6 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Activation</span>)            │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)                 │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dropout_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)                  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)                 │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dense_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">62</span>)                  │          <span style=\"color: #00af00; text-decoration-color: #00af00\">15,934</span> │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ activation_7 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Activation</span>)            │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">62</span>)                  │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m1,444,606\u001b[0m (5.51 MB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">1,444,606</span> (5.51 MB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m1,444,606\u001b[0m (5.51 MB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">1,444,606</span> (5.51 MB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Set the learning rate decay\n",
        "reduce_lr = ReduceLROnPlateau(monitor='loss', factor=0.2, patience=2, min_lr=0.001)"
      ],
      "metadata": {
        "id": "3ukMu0Kz918F"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Train\n",
        "H = alexnet.fit(\n",
        "    train_generator,\n",
        "    validation_data=val_generator,\n",
        "    steps_per_epoch=train_generator.samples // BATCH_SIZE,\n",
        "    validation_steps = val_generator.samples // BATCH_SIZE,\n",
        "    epochs=EPOCHS, verbose=1, callbacks=[reduce_lr])\n",
        "\n",
        "# Save\n",
        "# alexnet.save(MODEL_PATH + os.path.sep + \"trained_model\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "ZqjrhIRv95H8",
        "outputId": "1214da00-f220-4270-bb27-639ba9e37569"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[INFO] Training network ...\n",
            "Epoch 1/20\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/trainers/data_adapters/py_dataset_adapter.py:121: UserWarning: Your `PyDataset` class should call `super().__init__(**kwargs)` in its constructor. `**kwargs` can include `workers`, `use_multiprocessing`, `max_queue_size`. Do not pass these arguments to `fit()`, as they will be ignored.\n",
            "  self._warn_if_super_not_called()\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m651/651\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m56s\u001b[0m 72ms/step - accuracy: 0.3544 - loss: 2.4949 - val_accuracy: 0.9566 - val_loss: 0.1471 - learning_rate: 0.0010\n",
            "Epoch 2/20\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/lib/python3.10/contextlib.py:153: UserWarning: Your input ran out of data; interrupting training. Make sure that your dataset or generator can generate at least `steps_per_epoch * epochs` batches. You may need to use the `.repeat()` function when building your dataset.\n",
            "  self.gen.throw(typ, value, traceback)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\r\u001b[1m651/651\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.0000e+00 - loss: 0.0000e+00 - val_accuracy: 0.9375 - val_loss: 0.2144 - learning_rate: 0.0010\n",
            "Epoch 3/20\n",
            "\u001b[1m651/651\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m71s\u001b[0m 66ms/step - accuracy: 0.8972 - loss: 0.3274 - val_accuracy: 0.9737 - val_loss: 0.0919 - learning_rate: 0.0010\n",
            "Epoch 4/20\n",
            "\u001b[1m651/651\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 4ms/step - accuracy: 0.0000e+00 - loss: 0.0000e+00 - val_accuracy: 0.9792 - val_loss: 0.0680 - learning_rate: 0.0010\n",
            "Epoch 5/20\n",
            "\u001b[1m651/651\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 65ms/step - accuracy: 0.9447 - loss: 0.1771 - val_accuracy: 0.9730 - val_loss: 0.0972 - learning_rate: 0.0010\n",
            "Epoch 6/20\n",
            "\u001b[1m651/651\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 67us/step - accuracy: 0.0000e+00 - loss: 0.0000e+00 - val_accuracy: 0.9896 - val_loss: 0.0358 - learning_rate: 0.0010\n",
            "Epoch 7/20\n",
            "\u001b[1m651/651\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m82s\u001b[0m 64ms/step - accuracy: 0.9588 - loss: 0.1277 - val_accuracy: 0.9778 - val_loss: 0.0827 - learning_rate: 0.0010\n",
            "Epoch 8/20\n",
            "\u001b[1m651/651\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 66us/step - accuracy: 0.0000e+00 - loss: 0.0000e+00 - val_accuracy: 0.9688 - val_loss: 0.0492 - learning_rate: 0.0010\n",
            "Epoch 9/20\n",
            "\u001b[1m651/651\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 65ms/step - accuracy: 0.9680 - loss: 0.0973 - val_accuracy: 0.9783 - val_loss: 0.0807 - learning_rate: 0.0010\n",
            "Epoch 10/20\n",
            "\u001b[1m651/651\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 71us/step - accuracy: 0.0000e+00 - loss: 0.0000e+00 - val_accuracy: 0.9688 - val_loss: 0.1354 - learning_rate: 0.0010\n",
            "Epoch 11/20\n",
            "\u001b[1m651/651\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m85s\u001b[0m 69ms/step - accuracy: 0.9753 - loss: 0.0762 - val_accuracy: 0.9762 - val_loss: 0.0973 - learning_rate: 0.0010\n",
            "Epoch 12/20\n",
            "\u001b[1m651/651\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - accuracy: 0.0000e+00 - loss: 0.0000e+00 - val_accuracy: 0.9896 - val_loss: 0.0930 - learning_rate: 0.0010\n",
            "Epoch 13/20\n",
            "\u001b[1m651/651\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m44s\u001b[0m 68ms/step - accuracy: 0.9790 - loss: 0.0665 - val_accuracy: 0.9819 - val_loss: 0.0738 - learning_rate: 0.0010\n",
            "Epoch 14/20\n",
            "\u001b[1m651/651\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 82us/step - accuracy: 0.0000e+00 - loss: 0.0000e+00 - val_accuracy: 0.9896 - val_loss: 0.0497 - learning_rate: 0.0010\n",
            "Epoch 15/20\n",
            "\u001b[1m651/651\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m82s\u001b[0m 67ms/step - accuracy: 0.9801 - loss: 0.0630 - val_accuracy: 0.9760 - val_loss: 0.1058 - learning_rate: 0.0010\n",
            "Epoch 16/20\n",
            "\u001b[1m651/651\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 64us/step - accuracy: 0.0000e+00 - loss: 0.0000e+00 - val_accuracy: 0.9583 - val_loss: 0.1699 - learning_rate: 0.0010\n",
            "Epoch 17/20\n",
            "\u001b[1m651/651\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m82s\u001b[0m 67ms/step - accuracy: 0.9832 - loss: 0.0541 - val_accuracy: 0.9816 - val_loss: 0.0871 - learning_rate: 0.0010\n",
            "Epoch 18/20\n",
            "\u001b[1m651/651\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 100us/step - accuracy: 0.0000e+00 - loss: 0.0000e+00 - val_accuracy: 0.9479 - val_loss: 0.1643 - learning_rate: 0.0010\n",
            "Epoch 19/20\n",
            "\u001b[1m651/651\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m83s\u001b[0m 69ms/step - accuracy: 0.9833 - loss: 0.0523 - val_accuracy: 0.9801 - val_loss: 0.0886 - learning_rate: 0.0010\n",
            "Epoch 20/20\n",
            "\u001b[1m651/651\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 4ms/step - accuracy: 0.0000e+00 - loss: 0.0000e+00 - val_accuracy: 0.9792 - val_loss: 0.0776 - learning_rate: 0.0010\n",
            "[INFO] Serializing network...\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "Invalid filepath extension for saving. Please add either a `.keras` extension for the native Keras format (recommended) or a `.h5` extension. Use `model.export(filepath)` if you want to export a SavedModel for use with TFLite/TFServing/etc. Received: filepath=./trained_model.",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-11-f52ba88849d5>\u001b[0m in \u001b[0;36m<cell line: 12>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;31m# save the model to disk\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"[INFO] Serializing network...\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m \u001b[0malexnet\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mMODEL_PATH\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msep\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m\"trained_model\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"[INFO] Done!\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/src/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    120\u001b[0m             \u001b[0;31m# To get the full stack trace, call:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    121\u001b[0m             \u001b[0;31m# `keras.config.disable_traceback_filtering()`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 122\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    123\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    124\u001b[0m             \u001b[0;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/src/saving/saving_api.py\u001b[0m in \u001b[0;36msave_model\u001b[0;34m(model, filepath, overwrite, zipped, **kwargs)\u001b[0m\n\u001b[1;32m    107\u001b[0m             \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfilepath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moverwrite\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minclude_optimizer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    108\u001b[0m         )\n\u001b[0;32m--> 109\u001b[0;31m     raise ValueError(\n\u001b[0m\u001b[1;32m    110\u001b[0m         \u001b[0;34m\"Invalid filepath extension for saving. \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    111\u001b[0m         \u001b[0;34m\"Please add either a `.keras` extension for the native Keras \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: Invalid filepath extension for saving. Please add either a `.keras` extension for the native Keras format (recommended) or a `.h5` extension. Use `model.export(filepath)` if you want to export a SavedModel for use with TFLite/TFServing/etc. Received: filepath=./trained_model."
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "alexnet.save(MODEL_PATH + os.path.sep + \"trained_model.h5\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KMQMdTou0JMm",
        "outputId": "1f733a53-1941-4ac5-e9a8-ecccec1d128c"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model = keras.models.load_model(MODEL_PATH + os.path.sep + \"trained_model.h5\" )"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "52ZHrd2k0Szu",
        "outputId": "d0abe41e-fcf2-4dc8-e823-34a711643ee7"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from tensorflow.keras.preprocessing.image import load_img, img_to_array\n",
        "\n",
        "# Load the model\n",
        "model = keras.models.load_model(MODEL_PATH + os.path.sep + \"trained_model.h5\")\n",
        "\n",
        "# Load and preprocess the image\n",
        "def preprocess_image(image_path, target_size=(64, 64)):\n",
        "    # Load the image\n",
        "    img = load_img(image_path, target_size=target_size, color_mode='grayscale')\n",
        "    # Convert the image to an array\n",
        "    img_array = img_to_array(img)\n",
        "    # Scale pixel values to [0, 1]\n",
        "    img_array = img_array / 255.0\n",
        "    # Expand dimensions to match the input shape of the model\n",
        "    img_array = np.expand_dims(img_array, axis=0)\n",
        "    return img_array\n",
        "\n",
        "# Specify the path to the image you want to predict\n",
        "image_path = '/content/RanjanaDataset/Ranjana/Testing/15/15 (1).png'  # Update this with the actual image path\n",
        "processed_image = preprocess_image(image_path)\n",
        "\n",
        "# Make a prediction\n",
        "predictions = model.predict(processed_image)\n",
        "\n",
        "# Interpret the prediction results\n",
        "predicted_class = np.argmax(predictions, axis=1)\n",
        "print(f\"Predicted class: {predicted_class}\")\n",
        "\n",
        "\"\"\" The model works but the data generation is not in sequence, so the prediction is right\n",
        "for the sequence the folders are arranged as \"\"\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gAC0GL3c1qAQ",
        "outputId": "8bad8e19-a7c7-43a5-9b8d-be6e3576ceca"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 290ms/step\n",
            "Predicted class: [6]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# CapsuleNet"
      ],
      "metadata": {
        "id": "W09BZMAw3tma"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**For MNIST only**"
      ],
      "metadata": {
        "id": "VW75mHEX9BM0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# import resources\n",
        "import numpy as np\n",
        "import torch\n",
        "\n",
        "# random seed (for reproducibility)\n",
        "seed = 1\n",
        "# set random seed for numpy\n",
        "np.random.seed(seed)\n",
        "# set random seed for pytorch\n",
        "torch.manual_seed(seed)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j_Y8wx__3wPa",
        "outputId": "fa4955ff-abcd-4e3c-bec7-624f502246b1"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<torch._C.Generator at 0x7e99f5cccfd0>"
            ]
          },
          "metadata": {},
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from torchvision import datasets\n",
        "import torchvision.transforms as transforms\n",
        "\n",
        "# number of subprocesses to use for data loading\n",
        "num_workers = 0\n",
        "# how many samples per batch to load\n",
        "batch_size = 20\n",
        "\n",
        "# convert data to Tensors\n",
        "transform = transforms.ToTensor()\n",
        "\n",
        "# choose the training and test datasets\n",
        "train_data = datasets.MNIST(root='data', train=True,\n",
        "                            download=True, transform=transform)\n",
        "\n",
        "test_data = datasets.MNIST(root='data', train=False,\n",
        "                           download=True, transform=transform)\n",
        "\n",
        "# prepare data loaders\n",
        "train_loader = torch.utils.data.DataLoader(train_data,\n",
        "                                           batch_size=batch_size,\n",
        "                                           num_workers=num_workers)\n",
        "\n",
        "test_loader = torch.utils.data.DataLoader(test_data,\n",
        "                                          batch_size=batch_size,\n",
        "                                          num_workers=num_workers)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zKe0x9NC9FzY",
        "outputId": "53288eee-f4d6-4e28-bad9-dac4c1192690"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\n",
            "Failed to download (trying next):\n",
            "<urlopen error [SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: certificate has expired (_ssl.c:1007)>\n",
            "\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-images-idx3-ubyte.gz\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-images-idx3-ubyte.gz to data/MNIST/raw/train-images-idx3-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 9912422/9912422 [00:02<00:00, 4557154.76it/s] \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting data/MNIST/raw/train-images-idx3-ubyte.gz to data/MNIST/raw\n",
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz\n",
            "Failed to download (trying next):\n",
            "<urlopen error [SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: certificate has expired (_ssl.c:1007)>\n",
            "\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-labels-idx1-ubyte.gz\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-labels-idx1-ubyte.gz to data/MNIST/raw/train-labels-idx1-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 28881/28881 [00:00<00:00, 1806808.87it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting data/MNIST/raw/train-labels-idx1-ubyte.gz to data/MNIST/raw\n",
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Failed to download (trying next):\n",
            "<urlopen error [SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: certificate has expired (_ssl.c:1007)>\n",
            "\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-images-idx3-ubyte.gz\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-images-idx3-ubyte.gz to data/MNIST/raw/t10k-images-idx3-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1648877/1648877 [00:00<00:00, 3962709.72it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting data/MNIST/raw/t10k-images-idx3-ubyte.gz to data/MNIST/raw\n",
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz\n",
            "Failed to download (trying next):\n",
            "<urlopen error [SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: certificate has expired (_ssl.c:1007)>\n",
            "\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-labels-idx1-ubyte.gz\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-labels-idx1-ubyte.gz to data/MNIST/raw/t10k-labels-idx1-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 4542/4542 [00:00<00:00, 3911009.81it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting data/MNIST/raw/t10k-labels-idx1-ubyte.gz to data/MNIST/raw\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "\n",
        "# obtain one batch of training images\n",
        "dataiter = iter(train_loader)\n",
        "images, labels = next(dataiter)\n",
        "images = images.numpy()\n",
        "\n",
        "# plot the images in the batch, along with the corresponding labels\n",
        "fig = plt.figure(figsize=(25, 4))\n",
        "for idx in np.arange(batch_size):\n",
        "    ax = fig.add_subplot(2, batch_size//2, idx+1, xticks=[], yticks=[])\n",
        "    ax.imshow(np.squeeze(images[idx]), cmap='gray')\n",
        "    # print out the correct label for each image\n",
        "    # .item() gets the value contained in a Tensor\n",
        "    ax.set_title(str(labels[idx].item()))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 249
        },
        "id": "g_3BrDni9ggW",
        "outputId": "5ab5e9ac-e6d1-481c-ffa8-2648a67cd4f6"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 2500x400 with 20 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from torch import nn\n",
        "class ConvLayer(nn.Module):\n",
        "\n",
        "    def __init__(self, in_channels=1, out_channels=256):\n",
        "        '''Constructs the ConvLayer with a specified input and output size.\n",
        "           param in_channels: input depth of an image, default value = 1\n",
        "           param out_channels: output depth of the convolutional layer, default value = 256\n",
        "           '''\n",
        "        super(ConvLayer, self).__init__()\n",
        "\n",
        "        # defining a convolutional layer of the specified size\n",
        "        self.conv = nn.Conv2d(in_channels, out_channels,\n",
        "                              kernel_size=9, stride=1, padding=0)\n",
        "\n",
        "    def forward(self, x):\n",
        "        '''Defines the feedforward behavior.\n",
        "           param x: the input to the layer; an input image\n",
        "           return: a relu-activated, convolutional layer\n",
        "           '''\n",
        "        # applying a ReLu activation to the outputs of the conv layer\n",
        "        features = F.relu(self.conv(x)) # will have dimensions (batch_size, 20, 20, 256)\n",
        "        return features"
      ],
      "metadata": {
        "id": "qSBzaurF-JDn"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class PrimaryCaps(nn.Module):\n",
        "\n",
        "    def __init__(self, num_capsules=8, in_channels=256, out_channels=32):\n",
        "        '''Constructs a list of convolutional layers to be used in\n",
        "           creating capsule output vectors.\n",
        "           param num_capsules: number of capsules to create\n",
        "           param in_channels: input depth of features, default value = 256\n",
        "           param out_channels: output depth of the convolutional layers, default value = 32\n",
        "           '''\n",
        "        super(PrimaryCaps, self).__init__()\n",
        "\n",
        "        # creating a list of convolutional layers for each capsule I want to create\n",
        "        # all capsules have a conv layer with the same parameters\n",
        "        self.capsules = nn.ModuleList([\n",
        "            nn.Conv2d(in_channels=in_channels, out_channels=out_channels,\n",
        "                      kernel_size=9, stride=2, padding=0)\n",
        "            for _ in range(num_capsules)])\n",
        "\n",
        "    def forward(self, x):\n",
        "        '''Defines the feedforward behavior.\n",
        "           param x: the input; features from a convolutional layer\n",
        "           return: a set of normalized, capsule output vectors\n",
        "           '''\n",
        "        # get batch size of inputs\n",
        "        batch_size = x.size(0)\n",
        "        # reshape convolutional layer outputs to be (batch_size, vector_dim=1152, 1)\n",
        "        u = [capsule(x).view(batch_size, 32 * 6 * 6, 1) for capsule in self.capsules]\n",
        "        # stack up output vectors, u, one for each capsule\n",
        "        u = torch.cat(u, dim=-1)\n",
        "        # squashing the stack of vectors\n",
        "        u_squash = self.squash(u)\n",
        "        return u_squash\n",
        "\n",
        "    def squash(self, input_tensor):\n",
        "        '''Squashes an input Tensor so it has a magnitude between 0-1.\n",
        "           param input_tensor: a stack of capsule inputs, s_j\n",
        "           return: a stack of normalized, capsule output vectors, v_j\n",
        "           '''\n",
        "        squared_norm = (input_tensor ** 2).sum(dim=-1, keepdim=True)\n",
        "        scale = squared_norm / (1 + squared_norm) # normalization coeff\n",
        "        output_tensor = scale * input_tensor / torch.sqrt(squared_norm)\n",
        "        return output_tensor\n",
        ""
      ],
      "metadata": {
        "id": "vbJ3tVTB-Nd7"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#import helpers # to get transpose softmax function\n",
        "import torch.nn.functional as F\n",
        "\n",
        "# dynamic routing\n",
        "def dynamic_routing(b_ij, u_hat, squash, routing_iterations=3):\n",
        "    '''Performs dynamic routing between two capsule layers.\n",
        "       param b_ij: initial log probabilities that capsule i should be coupled to capsule j\n",
        "       param u_hat: input, weighted capsule vectors, W u\n",
        "       param squash: given, normalizing squash function\n",
        "       param routing_iterations: number of times to update coupling coefficients\n",
        "       return: v_j, output capsule vectors\n",
        "       '''\n",
        "    # update b_ij, c_ij for number of routing iterations\n",
        "    for iteration in range(routing_iterations):\n",
        "        # softmax calculation of coupling coefficients, c_ij\n",
        "        c_ij = F.softmax(b_ij, dim=2)\n",
        "\n",
        "        # calculating total capsule inputs, s_j = sum(c_ij*u_hat)\n",
        "        s_j = (c_ij * u_hat).sum(dim=2, keepdim=True)\n",
        "\n",
        "        # squashing to get a normalized vector output, v_j\n",
        "        v_j = squash(s_j)\n",
        "\n",
        "        # if not on the last iteration, calculate agreement and new b_ij\n",
        "        if iteration < routing_iterations - 1:\n",
        "            # agreement\n",
        "            a_ij = (u_hat * v_j).sum(dim=-1, keepdim=True)\n",
        "\n",
        "            # new b_ij\n",
        "            b_ij = b_ij + a_ij\n",
        "\n",
        "    return v_j # return latest v_j"
      ],
      "metadata": {
        "id": "TNCoJKWK-RlW"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# it will also be relevant, in this model, to see if I can train on gpu\n",
        "TRAIN_ON_GPU = torch.cuda.is_available()\n",
        "\n",
        "if(TRAIN_ON_GPU):\n",
        "    print('Training on GPU!')\n",
        "else:\n",
        "    print('Only CPU available')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hgPJLa4w-SI0",
        "outputId": "398eb63b-1d18-461f-bad7-6f9c44990716"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Only CPU available\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class DigitCaps(nn.Module):\n",
        "\n",
        "    def __init__(self, num_capsules=10, previous_layer_nodes=32*6*6,\n",
        "                 in_channels=8, out_channels=16):\n",
        "        '''Constructs an initial weight matrix, W, and sets class variables.\n",
        "           param num_capsules: number of capsules to create\n",
        "           param previous_layer_nodes: dimension of input capsule vector, default value = 1152\n",
        "           param in_channels: number of capsules in previous layer, default value = 8\n",
        "           param out_channels: dimensions of output capsule vector, default value = 16\n",
        "           '''\n",
        "        super(DigitCaps, self).__init__()\n",
        "\n",
        "        # setting class variables\n",
        "        self.num_capsules = num_capsules\n",
        "        self.previous_layer_nodes = previous_layer_nodes # vector input (dim=1152)\n",
        "        self.in_channels = in_channels # previous layer's number of capsules\n",
        "\n",
        "        # starting out with a randomly initialized weight matrix, W\n",
        "        # these will be the weights connecting the PrimaryCaps and DigitCaps layers\n",
        "        self.W = nn.Parameter(torch.randn(num_capsules, previous_layer_nodes,\n",
        "                                          in_channels, out_channels))\n",
        "\n",
        "    def forward(self, u):\n",
        "        '''Defines the feedforward behavior.\n",
        "           param u: the input; vectors from the previous PrimaryCaps layer\n",
        "           return: a set of normalized, capsule output vectors\n",
        "           '''\n",
        "\n",
        "        # adding batch_size dims and stacking all u vectors\n",
        "        u = u[None, :, :, None, :]\n",
        "        # 4D weight matrix\n",
        "        W = self.W[:, None, :, :, :]\n",
        "\n",
        "        # calculating u_hat = W*u\n",
        "        u_hat = torch.matmul(u, W)\n",
        "\n",
        "        # getting the correct size of b_ij\n",
        "        # setting them all to 0, initially\n",
        "        b_ij = torch.zeros(*u_hat.size())\n",
        "\n",
        "        # moving b_ij to GPU, if available\n",
        "        if TRAIN_ON_GPU:\n",
        "            b_ij = b_ij.cuda()\n",
        "\n",
        "        # update coupling coefficients and calculate v_j\n",
        "        v_j = dynamic_routing(b_ij, u_hat, self.squash, routing_iterations=3)\n",
        "\n",
        "        return v_j # return final vector outputs\n",
        "\n",
        "\n",
        "    def squash(self, input_tensor):\n",
        "        '''Squashes an input Tensor so it has a magnitude between 0-1.\n",
        "           param input_tensor: a stack of capsule inputs, s_j\n",
        "           return: a stack of normalized, capsule output vectors, v_j\n",
        "           '''\n",
        "        # same squash function as before\n",
        "        squared_norm = (input_tensor ** 2).sum(dim=-1, keepdim=True)\n",
        "        scale = squared_norm / (1 + squared_norm) # normalization coeff\n",
        "        output_tensor = scale * input_tensor / torch.sqrt(squared_norm)\n",
        "        return output_tensor"
      ],
      "metadata": {
        "id": "HN29oHBj-Uk8"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Decoder(nn.Module):\n",
        "\n",
        "    def __init__(self, input_vector_length=16, input_capsules=10, hidden_dim=512):\n",
        "        '''Constructs an series of linear layers + activations.\n",
        "           param input_vector_length: dimension of input capsule vector, default value = 16\n",
        "           param input_capsules: number of capsules in previous layer, default value = 10\n",
        "           param hidden_dim: dimensions of hidden layers, default value = 512\n",
        "           '''\n",
        "        super(Decoder, self).__init__()\n",
        "\n",
        "        # calculate input_dim\n",
        "        input_dim = input_vector_length * input_capsules\n",
        "\n",
        "        # define linear layers + activations\n",
        "        self.linear_layers = nn.Sequential(\n",
        "            nn.Linear(input_dim, hidden_dim), # first hidden layer\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Linear(hidden_dim, hidden_dim*2), # second, twice as deep\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Linear(hidden_dim*2, 28*28), # can be reshaped into 28*28 image\n",
        "            nn.Sigmoid() # sigmoid activation to get output pixel values in a range from 0-1\n",
        "            )\n",
        "\n",
        "    def forward(self, x):\n",
        "        '''Defines the feedforward behavior.\n",
        "           param x: the input; vectors from the previous DigitCaps layer\n",
        "           return: two things, reconstructed images and the class scores, y\n",
        "           '''\n",
        "        classes = (x ** 2).sum(dim=-1) ** 0.5\n",
        "        classes = F.softmax(classes, dim=-1)\n",
        "\n",
        "        # find the capsule with the maximum vector length\n",
        "        # here, vector length indicates the probability of a class' existence\n",
        "        _, max_length_indices = classes.max(dim=1)\n",
        "\n",
        "        # create a sparse class matrix\n",
        "        sparse_matrix = torch.eye(10) # 10 is the number of classes\n",
        "        if TRAIN_ON_GPU:\n",
        "            sparse_matrix = sparse_matrix.cuda()\n",
        "        # get the class scores from the \"correct\" capsule\n",
        "        y = sparse_matrix.index_select(dim=0, index=max_length_indices.data)\n",
        "\n",
        "        # create reconstructed pixels\n",
        "        x = x * y[:, :, None]\n",
        "        # flatten image into a vector shape (batch_size, vector_dim)\n",
        "        flattened_x = x.contiguous().view(x.size(0), -1)\n",
        "        # create reconstructed image vectors\n",
        "        reconstructions = self.linear_layers(flattened_x)\n",
        "\n",
        "        # return reconstructions and the class scores, y\n",
        "        return reconstructions, y"
      ],
      "metadata": {
        "id": "vNLazeBC-mhe"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class CapsuleNetwork(nn.Module):\n",
        "\n",
        "    def __init__(self):\n",
        "        '''Constructs a complete Capsule Network.'''\n",
        "        super(CapsuleNetwork, self).__init__()\n",
        "        self.conv_layer = ConvLayer()\n",
        "        self.primary_capsules = PrimaryCaps()\n",
        "        self.digit_capsules = DigitCaps()\n",
        "        self.decoder = Decoder()\n",
        "\n",
        "    def forward(self, images):\n",
        "        '''Defines the feedforward behavior.\n",
        "           param images: the original MNIST image input data\n",
        "           return: output of DigitCaps layer, reconstructed images, class scores\n",
        "           '''\n",
        "        primary_caps_output = self.primary_capsules(self.conv_layer(images))\n",
        "        caps_output = self.digit_capsules(primary_caps_output).squeeze().transpose(0,1)\n",
        "        reconstructions, y = self.decoder(caps_output)\n",
        "        return caps_output, reconstructions, y\n",
        ""
      ],
      "metadata": {
        "id": "LBfbsGz2-qBH"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# instantiate and print net\n",
        "capsule_net = CapsuleNetwork()\n",
        "\n",
        "print(capsule_net)\n",
        "\n",
        "# move model to GPU, if available\n",
        "if TRAIN_ON_GPU:\n",
        "    capsule_net = capsule_net.cuda()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2WddRqTJ-uTs",
        "outputId": "3c479ba2-ade7-4bc9-ad98-2559b6f03d8a"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CapsuleNetwork(\n",
            "  (conv_layer): ConvLayer(\n",
            "    (conv): Conv2d(1, 256, kernel_size=(9, 9), stride=(1, 1))\n",
            "  )\n",
            "  (primary_capsules): PrimaryCaps(\n",
            "    (capsules): ModuleList(\n",
            "      (0-7): 8 x Conv2d(256, 32, kernel_size=(9, 9), stride=(2, 2))\n",
            "    )\n",
            "  )\n",
            "  (digit_capsules): DigitCaps()\n",
            "  (decoder): Decoder(\n",
            "    (linear_layers): Sequential(\n",
            "      (0): Linear(in_features=160, out_features=512, bias=True)\n",
            "      (1): ReLU(inplace=True)\n",
            "      (2): Linear(in_features=512, out_features=1024, bias=True)\n",
            "      (3): ReLU(inplace=True)\n",
            "      (4): Linear(in_features=1024, out_features=784, bias=True)\n",
            "      (5): Sigmoid()\n",
            "    )\n",
            "  )\n",
            ")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class CapsuleLoss(nn.Module):\n",
        "\n",
        "    def __init__(self):\n",
        "        '''Constructs a CapsuleLoss module.'''\n",
        "        super(CapsuleLoss, self).__init__()\n",
        "        self.reconstruction_loss = nn.MSELoss(reduction='sum') # cumulative loss, equiv to size_average=False\n",
        "\n",
        "    def forward(self, x, labels, images, reconstructions):\n",
        "        '''Defines how the loss compares inputs.\n",
        "           param x: digit capsule outputs\n",
        "           param labels:\n",
        "           param images: the original MNIST image input data\n",
        "           param reconstructions: reconstructed MNIST image data\n",
        "           return: weighted margin and reconstruction loss, averaged over a batch\n",
        "           '''\n",
        "        batch_size = x.size(0)\n",
        "\n",
        "        ##  calculate the margin loss   ##\n",
        "\n",
        "        # get magnitude of digit capsule vectors, v_c\n",
        "        v_c = torch.sqrt((x**2).sum(dim=2, keepdim=True))\n",
        "\n",
        "        # calculate \"correct\" and incorrect loss\n",
        "        left = F.relu(0.9 - v_c).view(batch_size, -1)\n",
        "        right = F.relu(v_c - 0.1).view(batch_size, -1)\n",
        "\n",
        "        # sum the losses, with a lambda = 0.5\n",
        "        margin_loss = labels * left + 0.5 * (1. - labels) * right\n",
        "        margin_loss = margin_loss.sum()\n",
        "\n",
        "        ##  calculate the reconstruction loss   ##\n",
        "        images = images.view(reconstructions.size()[0], -1)\n",
        "        reconstruction_loss = self.reconstruction_loss(reconstructions, images)\n",
        "\n",
        "        # return a weighted, summed loss, averaged over a batch size\n",
        "        return (margin_loss + 0.0005 * reconstruction_loss) / images.size(0)"
      ],
      "metadata": {
        "id": "GpBj6YZ1-xSp"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.optim as optim\n",
        "\n",
        "# custom loss\n",
        "criterion = CapsuleLoss()\n",
        "\n",
        "# Adam optimizer with default params\n",
        "optimizer = optim.Adam(capsule_net.parameters())"
      ],
      "metadata": {
        "id": "DbDL7Afn-0Td"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train(capsule_net, criterion, optimizer,\n",
        "          n_epochs, print_every=300):\n",
        "    '''Trains a capsule network and prints out training batch loss statistics.\n",
        "       Saves model parameters if *validation* loss has decreased.\n",
        "       param capsule_net: trained capsule network\n",
        "       param criterion: capsule loss function\n",
        "       param optimizer: optimizer for updating network weights\n",
        "       param n_epochs: number of epochs to train for\n",
        "       param print_every: batches to print and save training loss, default = 100\n",
        "       return: list of recorded training losses\n",
        "       '''\n",
        "\n",
        "    # track training loss over time\n",
        "    losses = []\n",
        "\n",
        "    # one epoch = one pass over all training data\n",
        "    for epoch in range(1, n_epochs+1):\n",
        "\n",
        "        # initialize training loss\n",
        "        train_loss = 0.0\n",
        "\n",
        "        capsule_net.train() # set to train mode\n",
        "\n",
        "        # get batches of training image data and targets\n",
        "        for batch_i, (images, target) in enumerate(train_loader):\n",
        "\n",
        "            # reshape and get target class\n",
        "            target = torch.eye(10).index_select(dim=0, index=target)\n",
        "\n",
        "            if TRAIN_ON_GPU:\n",
        "                images, target = images.cuda(), target.cuda()\n",
        "\n",
        "            # zero out gradients\n",
        "            optimizer.zero_grad()\n",
        "            # get model outputs\n",
        "            caps_output, reconstructions, y = capsule_net(images)\n",
        "            # calculate loss\n",
        "            loss = criterion(caps_output, target, images, reconstructions)\n",
        "            # perform backpropagation and optimization\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            train_loss += loss.item() # accumulated training loss\n",
        "\n",
        "            # print and record training stats\n",
        "            if batch_i != 0 and batch_i % print_every == 0:\n",
        "                avg_train_loss = train_loss/print_every\n",
        "                losses.append(avg_train_loss)\n",
        "                print('Epoch: {} \\tTraining Loss: {:.8f}'.format(epoch, avg_train_loss))\n",
        "                train_loss = 0 # reset accumulated training loss\n",
        "\n",
        "    return losses"
      ],
      "metadata": {
        "id": "pyoiHeRv-3sG"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# training for 3 epochs\n",
        "n_epochs = 3\n",
        "losses = train(capsule_net, criterion, optimizer, n_epochs=n_epochs) # Lacks output visualization, bad UI to know if it is stuck or functioning as intended"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jV2Dnkjd-6Ss",
        "outputId": "984a4076-0f0b-47e8-e4b0-1b925a67fbd6"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 1 \tTraining Loss: 0.25191286\n",
            "Epoch: 1 \tTraining Loss: 0.09786229\n",
            "Epoch: 1 \tTraining Loss: 0.07618292\n",
            "Epoch: 1 \tTraining Loss: 0.06098419\n",
            "Epoch: 1 \tTraining Loss: 0.06087100\n",
            "Epoch: 1 \tTraining Loss: 0.05486129\n",
            "Epoch: 1 \tTraining Loss: 0.05166097\n",
            "Epoch: 1 \tTraining Loss: 0.05079275\n",
            "Epoch: 1 \tTraining Loss: 0.04839620\n",
            "Epoch: 2 \tTraining Loss: 0.04355478\n",
            "Epoch: 2 \tTraining Loss: 0.04127486\n",
            "Epoch: 2 \tTraining Loss: 0.03679715\n",
            "Epoch: 2 \tTraining Loss: 0.03551575\n",
            "Epoch: 2 \tTraining Loss: 0.03512019\n",
            "Epoch: 2 \tTraining Loss: 0.03390653\n",
            "Epoch: 2 \tTraining Loss: 0.03420521\n",
            "Epoch: 2 \tTraining Loss: 0.03365761\n",
            "Epoch: 2 \tTraining Loss: 0.03389532\n",
            "Epoch: 3 \tTraining Loss: 0.03092063\n",
            "Epoch: 3 \tTraining Loss: 0.02931087\n",
            "Epoch: 3 \tTraining Loss: 0.02812128\n",
            "Epoch: 3 \tTraining Loss: 0.02732869\n",
            "Epoch: 3 \tTraining Loss: 0.02823783\n",
            "Epoch: 3 \tTraining Loss: 0.02630499\n",
            "Epoch: 3 \tTraining Loss: 0.02781659\n",
            "Epoch: 3 \tTraining Loss: 0.02773226\n",
            "Epoch: 3 \tTraining Loss: 0.02718723\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "plt.plot(losses)\n",
        "plt.title(\"Training Loss\")\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 452
        },
        "id": "7FPm2wDX-7IB",
        "outputId": "ff55add8-3bb0-4957-b56b-2500fbbf23f3"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Try saving?\n",
        "torch.save(capsule_net.state_dict(),'model.pth')"
      ],
      "metadata": {
        "id": "shBI4SkC0jPU"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the saved weights\n",
        "capsule_net = CapsuleNetwork()\n",
        "# move model to GPU, if available\n",
        "if TRAIN_ON_GPU:\n",
        "  # Since it was trained on GPU, it is mapped as such\n",
        "  capsule_net.load_state_dict(torch.load('model.pth',weights_only = False))\n",
        "else:\n",
        "  capsule_net.load_state_dict(torch.load('model.pth',weights_only = False,map_location=torch.device('cpu')))\n",
        "capsule_net.eval()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Qey6f96vRHRd",
        "outputId": "1374ec3e-4c82-4f01-8cea-88147729fffb"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "CapsuleNetwork(\n",
              "  (conv_layer): ConvLayer(\n",
              "    (conv): Conv2d(1, 256, kernel_size=(9, 9), stride=(1, 1))\n",
              "  )\n",
              "  (primary_capsules): PrimaryCaps(\n",
              "    (capsules): ModuleList(\n",
              "      (0-7): 8 x Conv2d(256, 32, kernel_size=(9, 9), stride=(2, 2))\n",
              "    )\n",
              "  )\n",
              "  (digit_capsules): DigitCaps()\n",
              "  (decoder): Decoder(\n",
              "    (linear_layers): Sequential(\n",
              "      (0): Linear(in_features=160, out_features=512, bias=True)\n",
              "      (1): ReLU(inplace=True)\n",
              "      (2): Linear(in_features=512, out_features=1024, bias=True)\n",
              "      (3): ReLU(inplace=True)\n",
              "      (4): Linear(in_features=1024, out_features=784, bias=True)\n",
              "      (5): Sigmoid()\n",
              "    )\n",
              "  )\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def test(capsule_net, test_loader):\n",
        "    '''Prints out test statistics for a given capsule net.\n",
        "       param capsule_net: trained capsule network\n",
        "       param test_loader: test dataloader\n",
        "       return: returns last batch of test image data and corresponding reconstructions\n",
        "       '''\n",
        "    class_correct = list(0. for i in range(10))\n",
        "    class_total = list(0. for i in range(10))\n",
        "\n",
        "    test_loss = 0 # loss tracking\n",
        "\n",
        "    capsule_net.eval() # eval mode\n",
        "\n",
        "    for batch_i, (images, target) in enumerate(test_loader):\n",
        "        target = torch.eye(10).index_select(dim=0, index=target)\n",
        "\n",
        "        batch_size = images.size(0)\n",
        "\n",
        "        if TRAIN_ON_GPU:\n",
        "            images, target = images.cuda(), target.cuda()\n",
        "\n",
        "        # forward pass: compute predicted outputs by passing inputs to the model\n",
        "        caps_output, reconstructions, y = capsule_net(images)\n",
        "        # calculate the loss\n",
        "        loss = criterion(caps_output, target, images, reconstructions)\n",
        "        # update average test loss\n",
        "        test_loss += loss.item()\n",
        "        # convert output probabilities to predicted class\n",
        "        _, pred = torch.max(y.data.cpu(), 1)\n",
        "        _, target_shape = torch.max(target.data.cpu(), 1)\n",
        "\n",
        "        # compare predictions to true label\n",
        "        correct = np.squeeze(pred.eq(target_shape.data.view_as(pred)))\n",
        "        # calculate test accuracy for each object class\n",
        "        for i in range(batch_size):\n",
        "            label = target_shape.data[i]\n",
        "            class_correct[label] += correct[i].item()\n",
        "            class_total[label] += 1\n",
        "\n",
        "    # avg test loss\n",
        "    avg_test_loss = test_loss/len(test_loader)\n",
        "    print('Test Loss: {:.8f}\\n'.format(avg_test_loss))\n",
        "\n",
        "    for i in range(10):\n",
        "        if class_total[i] > 0:\n",
        "            print('Test Accuracy of %5s: %2d%% (%2d/%2d)' % (\n",
        "                str(i), 100 * class_correct[i] / class_total[i],\n",
        "                np.sum(class_correct[i]), np.sum(class_total[i])))\n",
        "        else:\n",
        "            print('Test Accuracy of %5s: N/A (no training examples)' % (classes[i]))\n",
        "\n",
        "    print('\\nTest Accuracy (Overall): %2d%% (%2d/%2d)' % (\n",
        "        100. * np.sum(class_correct) / np.sum(class_total),\n",
        "        np.sum(class_correct), np.sum(class_total)))\n",
        "\n",
        "    # return last batch of capsule vectors, images, reconstructions\n",
        "    return caps_output, images, reconstructions"
      ],
      "metadata": {
        "id": "1Gl2JGQg-_yr"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# call test function and get reconstructed images\n",
        "caps_output, images, reconstructions = test(capsule_net, test_loader)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Crduk6jJ_DDv",
        "outputId": "e59e5849-0655-4efc-dcf1-158955bc8439"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test Loss: 0.03409957\n",
            "\n",
            "Test Accuracy of     0: 99% (977/980)\n",
            "Test Accuracy of     1: 99% (1131/1135)\n",
            "Test Accuracy of     2: 98% (1018/1032)\n",
            "Test Accuracy of     3: 97% (989/1010)\n",
            "Test Accuracy of     4: 98% (965/982)\n",
            "Test Accuracy of     5: 98% (883/892)\n",
            "Test Accuracy of     6: 98% (947/958)\n",
            "Test Accuracy of     7: 99% (1023/1028)\n",
            "Test Accuracy of     8: 99% (965/974)\n",
            "Test Accuracy of     9: 98% (992/1009)\n",
            "\n",
            "Test Accuracy (Overall): 98% (9890/10000)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def test_single_image(capsule_net, image, criterion, target=None):\n",
        "    '''Classifies a single image using the trained capsule network.\n",
        "       param capsule_net: trained capsule network\n",
        "       param image: input image (should be a PyTorch tensor)\n",
        "       param criterion: loss function (optional for calculating loss)\n",
        "       param target: true label for the image (optional)\n",
        "       return: predicted class and reconstruction (if applicable)\n",
        "    '''\n",
        "    capsule_net.eval()  # Set to evaluation mode\n",
        "\n",
        "    # Add a batch dimension to the image tensor\n",
        "    image = image.unsqueeze(0)  # Shape: [1, C, H, W]\n",
        "\n",
        "    if TRAIN_ON_GPU:\n",
        "        image = image.cuda()\n",
        "\n",
        "    # Forward pass: compute predicted outputs by passing inputs to the model\n",
        "    with torch.no_grad():\n",
        "        caps_output, reconstructions, y = capsule_net(image)\n",
        "\n",
        "        if target is not None:\n",
        "            target = torch.eye(10).index_select(dim=0, index=target)\n",
        "            if TRAIN_ON_GPU:\n",
        "                target = target.cuda()\n",
        "\n",
        "            # Calculate loss\n",
        "            loss = criterion(caps_output, target, image, reconstructions)\n",
        "            print(f'Loss: {loss.item():.8f}')\n",
        "\n",
        "    # Convert output probabilities to predicted class\n",
        "    _, pred = torch.max(y.data.cpu(), 1)\n",
        "\n",
        "    predicted_class = pred.item()  # Get the predicted class as a scalar\n",
        "    print(f'Predicted class: {predicted_class}')\n",
        "\n",
        "    return predicted_class, reconstructions\n",
        "\n",
        "# Example usage\n",
        "# Assuming `image` is a pre-processed image tensor and `target` is the true label\n",
        "predicted_class, reconstructions = test_single_image(capsule_net, image, criterion, target)\n",
        "\n"
      ],
      "metadata": {
        "id": "m5CGrT4BGpSg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "***Reconstructing External Single Image Tester***"
      ],
      "metadata": {
        "id": "3k5T8chnXFKy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def test(capsule_net, test_loader):\n",
        "    '''Prints out test statistics for a given capsule net.\n",
        "       param capsule_net: trained capsule network\n",
        "       param test_loader: test dataloader\n",
        "       return: returns last batch of test image data and corresponding reconstructions\n",
        "       '''\n",
        "    class_correct = list(0. for i in range(10))\n",
        "    class_total = list(0. for i in range(10))\n",
        "\n",
        "    test_loss = 0 # loss tracking\n",
        "\n",
        "    capsule_net.eval() # eval mode\n",
        "\n",
        "    for batch_i, (images, target) in enumerate(test_loader):\n",
        "        target = torch.eye(10).index_select(dim=0, index=target)\n",
        "\n",
        "        batch_size = images.size(0)\n",
        "\n",
        "        if TRAIN_ON_GPU:\n",
        "            images, target = images.cuda(), target.cuda()\n",
        "\n",
        "        # forward pass: compute predicted outputs by passing inputs to the model\n",
        "        caps_output, reconstructions, y = capsule_net(images)\n",
        "        # calculate the loss\n",
        "        loss = criterion(caps_output, target, images, reconstructions)\n",
        "        # update average test loss\n",
        "        test_loss += loss.item()\n",
        "        # convert output probabilities to predicted class\n",
        "        _, pred = torch.max(y.data.cpu(), 1)\n",
        "        _, target_shape = torch.max(target.data.cpu(), 1)\n",
        "\n",
        "        # compare predictions to true label\n",
        "        correct = np.squeeze(pred.eq(target_shape.data.view_as(pred)))\n",
        "        # calculate test accuracy for each object class\n",
        "        for i in range(batch_size):\n",
        "            label = target_shape.data[i]\n",
        "            class_correct[label] += correct[i].item()\n",
        "            class_total[label] += 1\n",
        "\n",
        "    # avg test loss\n",
        "    avg_test_loss = test_loss/len(test_loader)\n",
        "    print('Test Loss: {:.8f}\\n'.format(avg_test_loss))\n",
        "\n",
        "    for i in range(10):\n",
        "        if class_total[i] > 0:\n",
        "            print('Test Accuracy of %5s: %2d%% (%2d/%2d)' % (\n",
        "                str(i), 100 * class_correct[i] / class_total[i],\n",
        "                np.sum(class_correct[i]), np.sum(class_total[i])))\n",
        "        else:\n",
        "            print('Test Accuracy of %5s: N/A (no training examples)' % (classes[i]))\n",
        "\n",
        "    print('\\nTest Accuracy (Overall): %2d%% (%2d/%2d)' % (\n",
        "        100. * np.sum(class_correct) / np.sum(class_total),\n",
        "        np.sum(class_correct), np.sum(class_total)))\n",
        "\n",
        "    # return last batch of capsule vectors, images, reconstructions\n",
        "    return caps_output, images, reconstructions"
      ],
      "metadata": {
        "id": "ZofGH1qoW_bv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def display_images(images, reconstructions):\n",
        "    '''Plot one row of original MNIST images and another row (below)\n",
        "       of their reconstructions.'''\n",
        "    # convert to numpy images\n",
        "    images = images.data.cpu().numpy()\n",
        "    reconstructions = reconstructions.view(-1, 1, 28, 28)\n",
        "    reconstructions = reconstructions.data.cpu().numpy()\n",
        "\n",
        "    # plot the first ten input images and then reconstructed images\n",
        "    fig, axes = plt.subplots(nrows=2, ncols=10, sharex=True, sharey=True, figsize=(26,5))\n",
        "\n",
        "    # input images on top row, reconstructions on bottom\n",
        "    for images, row in zip([images, reconstructions], axes):\n",
        "        for img, ax in zip(images, row):\n",
        "            ax.imshow(np.squeeze(img), cmap='gray')\n",
        "            ax.get_xaxis().set_visible(False)\n",
        "            ax.get_yaxis().set_visible(False)"
      ],
      "metadata": {
        "id": "V86It9ic_Fvt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# display original and reconstructed images, in rows\n",
        "display_images(images, reconstructions)\n"
      ],
      "metadata": {
        "id": "TecL-LFD_KNu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# convert data to Tensor *and* perform random affine transformation\n",
        "transform = transforms.Compose(\n",
        "    [transforms.RandomAffine(degrees=30, translate=(0.1,0.1)),\n",
        "     transforms.ToTensor()]\n",
        "    )\n",
        "\n",
        "# test dataset\n",
        "transformed_test_data = datasets.MNIST(root='data', train=False,\n",
        "                                       download=True, transform=transform)\n",
        "\n",
        "# prepare data loader\n",
        "transformed_test_loader = torch.utils.data.DataLoader(transformed_test_data,\n",
        "                                                      batch_size=batch_size,\n",
        "                                                      num_workers=num_workers)"
      ],
      "metadata": {
        "id": "CQGHhI_A_K28"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# obtain one batch of test images\n",
        "dataiter = iter(transformed_test_loader)\n",
        "images, labels = next(dataiter)\n",
        "images = images.numpy()\n",
        "\n",
        "# plot the images in the batch, along with the corresponding labels\n",
        "fig = plt.figure(figsize=(25, 4))\n",
        "for idx in np.arange(batch_size):\n",
        "    ax = fig.add_subplot(2, batch_size//2, idx+1, xticks=[], yticks=[])\n",
        "    ax.imshow(np.squeeze(images[idx]), cmap='gray')\n",
        "    # print out the correct label for each image\n",
        "    # .item() gets the value contained in a Tensor\n",
        "    ax.set_title(str(labels[idx].item()))"
      ],
      "metadata": {
        "id": "vAHxwHMz_Qms"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# call test function and get reconstructed images\n",
        "_, images, reconstructions = test(capsule_net, transformed_test_loader)"
      ],
      "metadata": {
        "id": "_cERAxbR_THz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# original input images\n",
        "display_images(images, reconstructions)"
      ],
      "metadata": {
        "id": "AbWZMFJk_VNr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def vector_analysis(capsule_net, x, select_idx=1):\n",
        "    '''Generates perturbed iage reconstructions given some digit capsule outputs.\n",
        "       param capsule_net: trained capsule network\n",
        "       param x: a batch of digit capsule outputs\n",
        "       param select_idx: selects which image in a batch to analyze, default = 1\n",
        "       return: list of perturbed, reconstructed images\n",
        "       '''\n",
        "\n",
        "    classes = (x ** 2).sum(dim=-1) ** 0.5\n",
        "    classes = F.softmax(classes, dim=-1)\n",
        "\n",
        "    # find the capsule with the maximum vector length\n",
        "    # here, vector length indicates the probability of a class' existence\n",
        "    _, max_length_indices = classes.max(dim=1)\n",
        "\n",
        "    # create a sparse class matrix\n",
        "    sparse_matrix = torch.eye(10) # 10 is the number of classes\n",
        "    if TRAIN_ON_GPU:\n",
        "        sparse_matrix = sparse_matrix.cuda()\n",
        "    # get the class scores from the \"correct\" capsule\n",
        "    y = sparse_matrix.index_select(dim=0, index=max_length_indices.data)\n",
        "\n",
        "    # create reconstructed pixels\n",
        "    x = x * y[:, :, None]\n",
        "\n",
        "    # flatten image into a vector shape (batch_size, vector_dim)\n",
        "    flattened_x = x.view(x.size(0), -1)\n",
        "    # select a single image from a batch to work with\n",
        "    flattened_x = flattened_x[select_idx]\n",
        "\n",
        "    # track reconstructed images\n",
        "    reconstructed_ims = []\n",
        "    # values to change *one* vector dimension by\n",
        "    perturb_range = np.arange(-0.25, 0.30, 0.05)\n",
        "\n",
        "    # iterate through 16 vector dims\n",
        "    for k in range(16):\n",
        "        # create a copy of flattened_x to modify\n",
        "        transformed_x = torch.zeros(*flattened_x.size()).cuda()\n",
        "        transformed_x[:] = flattened_x[:]\n",
        "        # iterate through each perturbation value\n",
        "        for j in range(len(perturb_range)):\n",
        "            # for each capsule output\n",
        "            for i in range(10):\n",
        "                transformed_x[k+(16*i)] = flattened_x[k+(16*i)]+perturb_range[j]\n",
        "\n",
        "            # create reconstructed images\n",
        "            reconstructions = capsule_net.decoder.linear_layers(transformed_x)\n",
        "            # reshape into 28x28 image, (batch_size, depth, x, y)\n",
        "            reconstructions = reconstructions.view(-1, 1, 28, 28)\n",
        "            reconstructed_ims.append(reconstructions)\n",
        "\n",
        "    # return final list of reconstructed ims\n",
        "    return reconstructed_ims"
      ],
      "metadata": {
        "id": "W6xiSYUP_X-K"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}